{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01098,
     "end_time": "2021-03-14T11:39:39.344488",
     "exception": false,
     "start_time": "2021-03-14T11:39:39.333508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References:\n",
    "- https://torchtext.readthedocs.io/en/latest/data.html\n",
    "- [Torchtext使用教程](https://blog.csdn.net/JWoswin/article/details/92821752)\n",
    "- [Pytorch学习记录-更深的TorchText学习01](https://www.jianshu.com/p/da3a5d5ed2ba)\n",
    "- [Kaggle Competetion: Seq2Seq Implementation - Failed Experiment](https://www.kaggle.com/mylee2009/seq2seq-implementation-failed-experiment)\n",
    "- https://zhuanlan.zhihu.com/p/87708546\n",
    "- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 数据预处理\n",
    "1. 读取数据\n",
    "2. 使用torchtext构建词表（转化为vector）\n",
    "3. 构建Iterator方便以batch进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "df['POI'] = df['POI/street'].str.extract(r'(.*)/', expand=True)\n",
    "df['street'] = df['POI/street'].str.extract(r'/(.*)', expand=True)\n",
    "df.sample(10)\n",
    "\n",
    "df.to_csv('train_POI.csv', columns=('id', 'raw_address', 'POI'), index=False)\n",
    "df.to_csv('train_street.csv', columns=('id', 'raw_address', 'street'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取，加入BOS和EOS字段，并增加valid.csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "df['POI'] = df['POI/street'].str.extract(r'(.*)/', expand=True)\n",
    "df['street'] = df['POI/street'].str.extract(r'/(.*)', expand=True)\n",
    "\n",
    "# df['POI'] = df['POI'].apply(lambda x : 'BOS '+ x + ' EOS')\n",
    "# df['street'] = df['street'].apply(lambda x : 'BOS '+ x + ' EOS')\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.9\n",
    "\n",
    "train = df[msk]\n",
    "val = df[~msk]\n",
    "\n",
    "train.to_csv('train_POI.csv', columns=('id', 'raw_address', 'POI'), index=False)\n",
    "train.to_csv('train_street.csv', columns=('id', 'raw_address', 'street'), index=False)\n",
    "val.to_csv('val_POI.csv', columns=('id', 'raw_address', 'POI'), index=False)\n",
    "val.to_csv('val_street.csv', columns=('id', 'raw_address', 'street'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据存储\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_POI = pd.read_csv(\"test_POIs.csv\")\n",
    "df_street = pd.read_csv(\"test_streets.csv\")\n",
    "\n",
    "POIs = df_POI['POI'].tolist()\n",
    "streets = df_street['street'].tolist()\n",
    "\n",
    "output = []\n",
    "for i in range(len(POIs)):\n",
    "    if pd.isnull(POIs[i]):\n",
    "        POI = ''\n",
    "    else:\n",
    "        POI = str(POIs[i])\n",
    "    if pd.isnull(streets[i]):\n",
    "        street = ''\n",
    "    else:\n",
    "    street = str(streets[i])\n",
    "    output.append({'id':i, 'POI/street': POI + '/' + street})\n",
    "\n",
    "df_output = pd.DataFrame(output)\n",
    "df_output.to_csv(\"test_answers.csv\", columns=('id','POI/street'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.构建词表和对应的train、validate、test数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jl', 'kapuk', 'timur', 'delta', 'sili', 'iii', 'lippo', 'cika', '11', 'a', 'cicau', 'cikarang', 'pusat'] []\n",
      "['toko', 'dita,', 'kertosono'] ['toko', 'dita']\n",
      "['s.', 'par', '53', 'sidanegara', '4', 'cilacap', 'tengah']\n",
      "[('no', 52513), ('rt', 45538), ('raya', 32701), ('1', 23636), ('2', 21756), ('rw', 21667), ('3', 18371), ('toko', 16667), ('4', 15146), ('barat', 14815)]\n"
     ]
    }
   ],
   "source": [
    "# for torchtext<=0.8.0\n",
    "# from torchtext.data import Field\n",
    "# from torchtext.data import TabularDataset\n",
    "# for torchtext>=0.9.0\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "# 建立trn、val、tst数据集\n",
    "tokenize=lambda x: x.split()\n",
    "TEXT=Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "\n",
    "tv_datafields=[\n",
    "    ('id',None),\n",
    "    ('raw_address',TEXT),\n",
    "    ('POI',TEXT)\n",
    "]\n",
    "trn,vld=TabularDataset.splits(\n",
    "    path=r'.',\n",
    "    train='train_POI.csv',\n",
    "    validation='val_POI.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=tv_datafields\n",
    ")\n",
    "\n",
    "tst_datafields=[\n",
    "    ('id',None),\n",
    "    ('raw_address',TEXT)\n",
    "]\n",
    "tst=TabularDataset(\n",
    "    path=r'test.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=tst_datafields\n",
    ")\n",
    "\n",
    "print(trn[0].raw_address, trn[0].POI)\n",
    "print(vld[1].raw_address, vld[1].POI)\n",
    "print(tst[0].raw_address)\n",
    "\n",
    "# 构建词表\n",
    "# TEXT.build_vocab(trn)\n",
    "TEXT.build_vocab(trn,val,tst)\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.建立Iterator方便以batch训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.legacy.data.batch.Batch of size 64]\n",
       "\t[.raw_address]:[torch.LongTensor of size 16x64]\n",
       "\t[.POI]:[torch.LongTensor of size 7x64]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.legacy.data import Iterator, BucketIterator\n",
    "\n",
    "# sort_key就是告诉BucketIterator使用哪个key值去进行组合，很明显，在这里是comment_text\n",
    "# repeat设定为False是因为之后要打包这个迭代层\n",
    "# train_iter, val_iter=BucketIterator.splits(\n",
    "#     (trn,vld),\n",
    "#     batch_sizes=(64,64),\n",
    "#     device=DEVICE,\n",
    "#     sort_key=lambda x:len(x.raw_address),\n",
    "#     sort_within_batch=False,\n",
    "#     repeat=False\n",
    "# )\n",
    "\n",
    "train_iter=Iterator(\n",
    "    trn,\n",
    "    batch_size=64,\n",
    "    device=DEVICE,\n",
    "    sort_key=lambda x:len(x.raw_address),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False\n",
    ")\n",
    "\n",
    "val_iter=Iterator(\n",
    "    vld,\n",
    "    batch_size=64,\n",
    "    device=DEVICE,\n",
    "    sort_key=lambda x:len(x.raw_address),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False\n",
    ")\n",
    "\n",
    "test_iter = Iterator(\n",
    "    tst, \n",
    "    batch_size=64, \n",
    "    device=DEVICE, \n",
    "    sort=False, \n",
    "    sort_within_batch=False, \n",
    "    repeat=False\n",
    ")\n",
    "\n",
    "# 显示一个batch的结构\n",
    "batch=next(train_iter.__iter__());batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   120,    969,    321,   1667,   1899,   3015,     57,   4042,   1389,\n",
       "            7472,   1363,  10310,     20,    239,    511,     74,     42,    393,\n",
       "             165,   3290,    311,  13113,   4767,   2770,     42,   1422,     63,\n",
       "             744,  53226,     42,     17,    533,    141,   1142,  12395,      4,\n",
       "           21514,   1082,   7480,    765,   1266,  12355,     60,    269,   1857,\n",
       "            3827,   3733,     53,    508,     82,    666,      4,     71,    947,\n",
       "              75,  43942,   8965,    587,    251,   1660,    750,    361,    182,\n",
       "          133730],\n",
       "         [   993,   1308,     40,     94,   1737,  26091,     73,      2,   5891,\n",
       "              41,  28830,    508,  26759,   1739,     16,   4514,  43730,   6047,\n",
       "             235,    588,   6282,    181,    932,   3972,  14991,   1378,   1036,\n",
       "               4,    114,  49963,   4358,  99370,  13692,    113,    291,  42180,\n",
       "              11,   3050,    536,   1393,     46,   7954,  14690,     32,    685,\n",
       "           10976,   1703,    334,    466,  55184,     60,  16070,    516,   2032,\n",
       "              30,     60,     25,     52,   9266,      4,    252,   1035,     55,\n",
       "            1183],\n",
       "         [  1146,     59,    678,    449,    554,     38,   3254,    178,   5201,\n",
       "            1465,    386,    918,  15927,     30,     38,     16,  14238,  35617,\n",
       "               2,  10849,  15298,      6,    110,      2,   1520,    184,      1,\n",
       "             396,   1236,   1052,    183,  42585,    898,    778,  11275,  19532,\n",
       "              30,     28,   5339,     18,    117,    383,    768,   1804,    656,\n",
       "            9608,      1,   1682,   4432,    271,   3703,      2,  13435,    446,\n",
       "              59,   1857,    165,    349,    127,   2149,     37,   2471,    809,\n",
       "            1845],\n",
       "         [  1848,   4787,   5566,      1,    288,    984,    110,  10140,  17278,\n",
       "              31,   3081,   5265,     82,    586,   5044,     56,    741,      2,\n",
       "              49,      2,      1,    244,   9718,  13907,  29346,  17979,      1,\n",
       "              36,      1,    892,   4524,   1495,    956,    255,  34787,   5973,\n",
       "             160,  11179,      1, 128543,    285,    783,      3,     56, 136363,\n",
       "               1,      1,    266,   7990,    381,   5725,   1211,    981,    677,\n",
       "            1567,    927,     51,      1,     28,     36,    436,     28,  15839,\n",
       "               1],\n",
       "         [     4,    185,   7159,      1,  39092,    117,     70,     15,      1,\n",
       "             146,   4081,      1,  11056,     37,      3,    379,    365,    264,\n",
       "               3,  37239,      1,     11,      3,   1714,     64,    573,      1,\n",
       "              72,      1,    155,      1,  71500,      1,      3,  52142,      1,\n",
       "            1189,  29504,      1,  15547,    213,   6639,      5,   1559,    535,\n",
       "               1,      1,     63,   1236,  19432,     21,    499,    246,   3672,\n",
       "              23,  46412,    390,      1,     69,   5268,      1,  92452,  34020,\n",
       "               1],\n",
       "         [ 25962,      1,   7517,      1,    208,    941,      3,      1,      1,\n",
       "               1,  10184,      1,  65055,   2426,     26,     54,    376,   5769,\n",
       "              23,    476,      1,    244,     26,   8770,    209,    136,      1,\n",
       "             308,      1,    389,      1,  19071,      1,     18,     91,      1,\n",
       "               1,     35,      1,   8358,     26,      1,     34,      3,   1552,\n",
       "               1,      1,     94,      1,  45615,   7852,      1,   1305,      1,\n",
       "              89,     86,     25,      1,      2,      2,      1,     20,  11026,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,   1161,   2802,     23,      1,      1,\n",
       "               1,     41,      1,      1,      1,     10,     56,   1303,   5053,\n",
       "              34,      1,      1,      1,      6,    540,   1799,     16,      1,\n",
       "               3,      1,     86,      1,      1,      1,     14,  24701,      1,\n",
       "               1,      1,      1,      1,      3,      1,    116,     49,      1,\n",
       "               1,      1,      1,      1,   6411,    212,      1,      1,      1,\n",
       "             276,   8593,      1,      1,      8,    909,      1,    979,     16,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,    209,    391,      7,      1,      1,\n",
       "               1,    764,      1,      1,      1,    198,  48178,      2,    740,\n",
       "              67,      1,      1,      1,      1,      7,     61,      1,      1,\n",
       "              18,      1,   9556,      1,      1,      1,    125,      1,      1,\n",
       "               1,      1,      1,      1,      8,      1,     16,      6,      1,\n",
       "               1,      1,      1,      1,     11,    549,      1,      1,      1,\n",
       "               1,      1,      1,      1,    105,      3,      1,   4228,   3487,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,  46933,    602,     14,      1,      1,\n",
       "               1,      5,      1,      1,      1,      1,  90194,   1074,     11,\n",
       "              46,      1,      1,      1,      1,     10,    285,      1,      1,\n",
       "               6,      1,   1272,      1,      1,      1,    259,      1,      1,\n",
       "               1,      1,      1,      1,     56,      1,      1,    401,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,  41801,      6,      1,   8483,      1,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,      1,      5,   1645,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,     91,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              29,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,   4962,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,    700,   1960,      1,    491,      1,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,      1,      3,     57,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,  19749,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             484,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,     45,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,   9266,      1,      1,  10376,      1,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,      1,     23,     73,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,  23641,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,     16,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,      1,      6,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,      1,    106,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1]]),\n",
       " tensor([[     1,   4787,    321,      1,   1899,    788,      1,      1,   1389,\n",
       "               1,   1363,      1,     82,      1,      1,    379,     42,      1,\n",
       "               1,      1,    311,      1,      1,   1714,     42,      1,      1,\n",
       "               1,  53226,     42,      1,  32201,      1,      1,    291,      1,\n",
       "               1,   1082, 133839,      1,      1,  12355,      1,      1,   1857,\n",
       "            3827,   1833,     53,      1,    381,      1,      1,     71,    947,\n",
       "               1,      1,      1,      1,    251,      1,      1,    361,    182,\n",
       "               1],\n",
       "         [     1,      1,     40,      1,   1737,    391,      1,      1,   3154,\n",
       "               1,  12865,      1,  11056,      1,      1,     54,  43730,      1,\n",
       "               1,      1,   6282,      1,      1,   7426,  14991,      1,      1,\n",
       "               1,    114,  49963,      1,    509,      1,      1,  11275,      1,\n",
       "               1,   3050,      1,      1,      1,   1963,      1,      1,    685,\n",
       "            5080,      1,    334,      1,  19432,      1,      1,    516,    428,\n",
       "               1,      1,      1,      1,   9266,      1,      1,   1035,     55,\n",
       "               1],\n",
       "         [     1,      1, 107226,      1,    554,    602,      1,      1,   5201,\n",
       "               1,   5151,      1,      1,      1,      1,     56,   5373,      1,\n",
       "               1,      1,  36070,      1,      1,    142,   1520,      1,      1,\n",
       "               1,      1,   1052,      1,      1,      1,      1,  34787,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,    656,\n",
       "               1,      1,   1682,      1,      1,      1,      1,  28303,      1,\n",
       "               1,      1,      1,      1,    127,      1,      1,   2471,    809,\n",
       "               1],\n",
       "         [     1,      1,   7292,      1,    288,      5,      1,      1, 137207,\n",
       "               1,   3081,      1,      1,      1,      1,      1,    156,      1,\n",
       "               1,      1,      1,      1,      1,      1,  12376,      1,      1,\n",
       "               1,      1,    892,      1,      1,      1,      1,  52142,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,  69126,\n",
       "               1,      1,    266,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,  15839,\n",
       "               1],\n",
       "         [     1,      1,   1126,      1,  24862,      1,      1,      1,      1,\n",
       "               1,   1767,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,     91,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1],\n",
       "         [     1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,  24701,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BucketIterator或Iterator返回一个名为torchtext.data.Batch的自定义数据类型\n",
    "# 这使得代码重用变得困难（因为每次列名更改时，我们都需要修改代码）\n",
    "# 并且使得torchtext很难与其他库一起用于某些用例（例如torchsample和fastai）\n",
    "# 因此定义一个Iterator的包装器\n",
    "\n",
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        # self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "        self.dl, self.x_var, self.y_var = dl, x_var, y_vars\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            if self.y_var is not None: # we will concatenate y into a single tensor\n",
    "                # y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "                y = getattr(batch, self.y_var)\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            \n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iter, \"raw_address\", \"POI\")\n",
    "valid_dl = BatchWrapper(val_iter, \"raw_address\", \"POI\")\n",
    "test_dl = BatchWrapper(test_iter, \"raw_address\", None)\n",
    "\n",
    "# 显示一个batch的数据\n",
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141476"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. LSTM分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, in_size, hid_size=256, emb_size=256):\n",
    "        super().__init__()\n",
    "        self.embedding == nn.Embedding(in_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hid_size)\n",
    "        \n",
    "    def forward(self, inp, hid, cel):\n",
    "        embedded = self.embedding(inp)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hid, cel))\n",
    "        return output, hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, out_size, hid_size=256, emb_size=256, drop_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(out_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hid_size)\n",
    "        self.out = nn.Linear(hid_size, out_size)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "    \n",
    "    def forward(self, inp, hid, cel):\n",
    "        # embedded = self.embedding(inp)\n",
    "        embedded = F.relu(self.embedding(inp))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hid, cel))\n",
    "        output = self.out(output)\n",
    "        output = self.dropout(output)\n",
    "        return output, hidden, cell\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, in_out_size, hid_size, emb_size, drop_rate):\n",
    "        self.encoder = EncoderLSTM(in_out_size)\n",
    "        self.decoder = DecoderLSTM(in_out_size)\n",
    "    \n",
    "    def forward(self, seq, hid, cel):\n",
    "        output, hidden, cell = self.encoder(seq, hid, cel)\n",
    "        output hidden, cell = self.decoder(seq, hidden, cell)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden) # 上一个unit的output和hidden到这一个unit来\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # context = [n layers, batch size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        # input = [1, batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        # embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        # output = [sent len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # !! sent len and n directions will always be 1 in the decoder,therefore:\n",
    "\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "\n",
    "        prediction = self.out(output.squeeze(0))\n",
    "\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "    # seq2seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [src sent len, batch size]\n",
    "        # trg = [trg sent len, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size,\n",
    "                              trg_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "\n",
    "        hidden, cell = self.encoder.forward(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "\n",
    "            output, hidden, cell = self.decoder.forward(input, hidden, cell)\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, 203 use predicted token\n",
    "            # 在 模型训练速度 和 训练测试差别不要太大 作一个均衡\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300, num_linear=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1)\n",
    "        self.linear_layers = []\n",
    "\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linear_layer = nn.ModuleList(self.linear_layers)\n",
    "\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "\n",
    "        return preds\n",
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = LSTM(nh, emb_dim=em_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "opt=optim.Adam(model.parameters(),lr=1e-2)\n",
    "loss_func=nn.BCEWithLogitsLoss()\n",
    "epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train()\n",
    "    for x, y in tqdm.tqdm(train_dl):\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item()* x.size(0)\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model.eval()  # 评估模式\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        val_loss += loss.item()* x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n",
    "test_preds = []\n",
    "for x, y in tqdm.tqdm(test_dl):\n",
    "    preds = model(x)\n",
    "    preds = preds.data.numpy()\n",
    "    # 模型的实际输出是logit，所以再经过一个sigmoid函数\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "    test_preds = np.hstack(test_preds)\n",
    "\n",
    "print(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.762227,
     "end_time": "2021-03-14T11:44:26.571977",
     "exception": false,
     "start_time": "2021-03-14T11:44:25.809750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 295.878004,
   "end_time": "2021-03-14T11:44:30.107125",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-14T11:39:34.229121",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
