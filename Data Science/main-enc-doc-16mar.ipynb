{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "labeled-piano"
      },
      "source": [
        "### 数据读取"
      ],
      "id": "labeled-piano"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expensive-aruba"
      },
      "source": [
        "数据格式\n",
        "\n",
        "`raw_addr_words_train` and `raw_addr_words_test`: [['s.', 'par', '53', 'sidanegara', '4', 'cilacap', 'tengah', 'BOS'], ['angg', 'per,', 'baloi', 'indah', 'kel.', 'lubuk', 'baja', 'BOS'], ['asma', 'laun,', 'mand', 'imog,', 'BOS']]\n",
        "\n",
        "`POIs_words` and `streets_words`: [['kakap', 'raya', 'EOS'], ['jend', 'ahmad', 'yani', 'EOS'], ['raya', 'cila', 'kko', 'EOS'], ['EOS']]\n",
        "\n",
        "`sents` concatenates `raw_addr_words_train`, `raw_addr_words_test`, `POIs_words` and `streets_words`."
      ],
      "id": "expensive-aruba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "opposite-housing",
        "outputId": "d907ce65-b537-4b9a-b2c4-35fea102185b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_train.head()\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "df_test.head()\n",
        "\n",
        "# 处理raw_addr_words_train和raw_addr_words_test：split和append('BOS')\n",
        "raw_addr_train = df_train['raw_address'].tolist()\n",
        "raw_addr_test = df_test['raw_address'].tolist()\n",
        "raw_addr_words_train = []\n",
        "raw_addr_words_test = []\n",
        "for sentence in raw_addr_train:\n",
        "    sentence = sentence.replace(\",\", \"\")\n",
        "    sentence = sentence.split()\n",
        "    sentence.append('BOS')\n",
        "    raw_addr_words_train.append(sentence)\n",
        "for sentence in raw_addr_test:\n",
        "    sentence = sentence.replace(\",\", \"\")\n",
        "    sentence = sentence.split()\n",
        "    sentence.append('BOS')\n",
        "    raw_addr_words_test.append(sentence)\n",
        "\n",
        "# 把raw_addr_words_train和raw_addr_words_test都添加到sents\n",
        "sents = raw_addr_words_train\n",
        "sents.extend(raw_addr_words_test)\n",
        "\n",
        "# 求出input中sentence最长的长度，作为LSTM的time_step的参考\n",
        "max_sentence_length = 0\n",
        "for sentence in sents:\n",
        "    if len(sentence) > max_sentence_length:\n",
        "        max_sentence_length = len(sentence)\n",
        "\n",
        "# 处理POIs_words和streets_words：split和append('EOS')\n",
        "POIs_words = []\n",
        "streets_words = []\n",
        "for gt in df_train['POI/street'].tolist():\n",
        "    POI = gt.split('/', 1)[0]\n",
        "    street = gt.split('/', 1)[1]\n",
        "    POI = POI.split()\n",
        "    POI.append('EOS')\n",
        "    street = street.split()\n",
        "    street.append('EOS')\n",
        "    POIs_words.append(POI)\n",
        "    streets_words.append(street)\n",
        "\n",
        "# 求出ground_truth中sentence最长的长度，作为LSTM的time_step的参考\n",
        "max_poi_length = 0\n",
        "for sentence in POIs_words:\n",
        "    if len(sentence) > max_poi_length:\n",
        "        max_poi_length = len(sentence)\n",
        "max_street_length = 0\n",
        "for sentence in streets_words:\n",
        "    if len(sentence) > max_street_length:\n",
        "        max_street_length = len(sentence)\n",
        "\n",
        "# 把POIs_words和streets_words都添加到sents\n",
        "sents.extend(POIs_words)\n",
        "sents.extend(streets_words)\n",
        "\n",
        "print(sents[:10])\n",
        "print(sents[-10:])\n",
        "print(len(sents))\n",
        "print(\"max_sentence_length: \", max_sentence_length)\n",
        "print(\"max_poi_length: \", max_poi_length)\n",
        "print(\"max_street_length: \", max_street_length)\n",
        "# 综合考虑，time_step为64就够用\n",
        "\n",
        "# df_words = pd.DataFrame()"
      ],
      "id": "opposite-housing",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['jl', 'kapuk', 'timur', 'delta', 'sili', 'iii', 'lippo', 'cika', '11', 'a', 'cicau', 'cikarang', 'pusat', 'BOS'], ['aye', 'jati', 'sampurna', 'BOS'], ['setu', 'siung', '119', 'rt', '5', '1', '13880', 'cipayung', 'BOS'], ['toko', 'dita', 'kertosono', 'BOS'], ['jl.', 'orde', 'baru', 'BOS'], ['raya', 'samb', 'gede', '299', 'toko', 'bb', 'kids', 'BOS'], ['kem', 'mel', 'raya', 'no', '4', 'bojong', 'rawalumbu', 'rt', '1', '36', 'rawalumbu', 'BOS'], ['tela', 'keuramat', 'kuta', 'alam', 'BOS'], ['gg.', 'i', 'wates', 'magersari', 'BOS'], ['bunga', 'ncole', 'ix', '2', 'BOS']]\n",
            "[['EOS'], ['prib', '3', 'EOS'], ['EOS'], ['perum', 'tata', 'resid', 'nirwana', 'EOS'], ['kakap', 'raya', 'EOS'], ['jend', 'ahmad', 'yani', 'EOS'], ['raya', 'cila', 'kko', 'EOS'], ['EOS'], ['EOS'], ['EOS']]\n",
            "950000\n",
            "max_sentence_length:  33\n",
            "max_poi_length:  21\n",
            "max_street_length:  16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interesting-mortgage"
      },
      "source": [
        "### Word2Vec"
      ],
      "id": "interesting-mortgage"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aerial-lincoln"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# https://radimrehurek.com/gensim/models/word2vec.html\n",
        "# 一般小语料库的vector维度用200-300\n",
        "# sentences (iterable of iterables, optional) – 供训练的句子，可以使用简单的列表，但是对于大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。\n",
        "# corpus_file (str, optional) – LineSentence格式的语料库文件路径。\n",
        "# size (int, optional) – word向量的维度。\n",
        "# window (int, optional) – 一个句子中当前单词和被预测单词的最大距离。\n",
        "# min_count (int, optional) – 忽略词频小于此值的单词。\n",
        "# workers (int, optional) – 训练模型时使用的线程数。\n",
        "\n",
        "model = gensim.models.Word2Vec(sentences=sents, size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")\n",
        "# model.load(\"word2vec.model\")"
      ],
      "id": "aerial-lincoln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "absolute-gateway",
        "outputId": "4c177ba1-dca4-4fc8-b727-178beb4f2638"
      },
      "source": [
        "print(model.wv['<BOS>'])\n",
        "print(model.wv['<EOS>'])\n",
        "print(model.wv['kapuk'])\n",
        "print(model.similarity('kapuk','timur'))\n",
        "print(model.similarity('yaya','yayasan'))"
      ],
      "id": "absolute-gateway",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-4.7510145e-03 -4.7370796e-03  2.9411297e-03  4.8859799e-03\n",
            "  2.8488715e-03 -2.6719654e-03  3.6631844e-03  2.3172596e-03\n",
            "  2.5307646e-03 -2.2685218e-03  1.7151979e-04  3.7271092e-03\n",
            " -3.4571113e-03  1.6590450e-03 -1.7351321e-03  3.1850750e-03\n",
            " -4.2197909e-03  3.4302576e-03 -1.9578892e-03  3.5440323e-03\n",
            "  3.2221025e-04  1.5887568e-03  3.7067465e-03  1.3883339e-03\n",
            " -2.5920195e-03 -9.9590898e-04 -3.0845886e-03  3.4212242e-03\n",
            " -3.2305140e-03  2.5452189e-03 -4.3210844e-04  4.2407182e-03\n",
            "  4.6338956e-03 -8.9179759e-04 -4.9878997e-03 -4.3554399e-03\n",
            " -3.0786749e-03  2.9000796e-03 -2.4078618e-05  7.2029402e-04\n",
            " -1.3743703e-03 -1.7121357e-03  3.6064379e-03  4.6777684e-04\n",
            "  3.5667121e-03 -2.5101295e-03 -3.6852087e-03  2.1823538e-03\n",
            " -3.2539712e-04 -4.3696836e-03 -1.6757927e-03 -1.2041670e-03\n",
            " -2.9920565e-03  1.9305609e-03 -1.3281346e-03  1.8970171e-03\n",
            " -1.9230577e-03  2.5611972e-03  4.3853882e-04  1.3362422e-03\n",
            " -3.8731010e-03  6.1253677e-06  1.9853523e-03  2.8401176e-03\n",
            "  9.6844560e-05 -3.5904753e-03 -4.5014014e-03 -3.3059060e-03\n",
            "  1.5304619e-03  2.0367226e-03 -3.7803354e-03 -1.8748158e-03\n",
            " -2.7079561e-03  7.6408027e-04  4.9496009e-03  3.3538127e-03\n",
            "  2.5702813e-03 -2.5026947e-03 -2.8639436e-03  4.2784857e-03\n",
            "  8.7097380e-04  9.1154140e-04 -3.3053458e-03  2.7994537e-03\n",
            " -2.9586896e-03 -7.3931238e-04  4.6216282e-03  4.9392512e-04\n",
            "  2.4397911e-03 -1.2096054e-03 -3.2304409e-03 -5.8342074e-04\n",
            " -1.0712136e-03 -3.5419292e-03  4.7687837e-03 -2.1839654e-03\n",
            "  2.8180857e-03 -2.6254398e-03 -4.9907607e-03 -3.8439367e-04]\n",
            "[ 0.7792587  -0.3186208   1.0165286  -1.603868    0.42817318 -0.90469694\n",
            " -2.0678914  -1.4848149   0.2286407  -2.52006     0.47171396  2.1091104\n",
            "  1.6222702  -0.08316635 -1.1231011   0.6846474  -0.2827008  -1.0943142\n",
            " -1.4110168   0.5549853  -0.966215   -0.6828998   0.46646816  1.9229639\n",
            " -2.1462157   0.4239478  -1.2549305   1.6505171   2.8565779  -2.0522437\n",
            "  0.18164109  1.6721239  -2.7316792   3.4570484   1.2108082  -1.4716952\n",
            "  1.6487671  -0.6563985  -1.4167942  -0.77016044 -0.21866955  2.0197825\n",
            " -1.0340664   0.15723717 -0.76958585 -2.888719    1.2837292   0.13297488\n",
            " -0.98335874  0.3367005   0.9902607  -0.10821269  1.8216987   0.91281575\n",
            " -1.8765095   0.45936587 -1.9085863   0.5008153   0.07053899 -0.18178298\n",
            "  0.8085123  -0.49125186  1.0233546  -0.05948103 -0.55223954  0.5628268\n",
            " -0.54080194 -0.36044362 -0.21351016  1.0512823  -1.726516   -0.6820916\n",
            " -1.2083191  -1.438667   -0.69042826  0.7273151  -0.8149152   1.6022135\n",
            "  2.1129084   1.6585091  -0.5798151   0.47931093  2.135749   -0.22074656\n",
            " -1.8221859   1.6849693   0.6074943   0.34337264  2.779183    0.27650484\n",
            "  2.1257515  -0.25827563  0.1124647   0.38945076 -2.2956471   1.4768062\n",
            "  0.2852042   0.4184854  -1.2105635   0.59952265]\n",
            "0.34203595\n",
            "0.8780363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complex-production"
      },
      "source": [
        "### 预处理"
      ],
      "id": "complex-production"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boolean-antarctica",
        "outputId": "9efa0839-e775-4511-a3eb-cdbbc2ffd12c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# raw_addr_words_train\n",
        "# raw_addr_words_test\n",
        "# POIs_words\n",
        "# streets_words\n",
        "\n",
        "def word2idx(word):\n",
        "    return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "    return word_model.wv.index2word[idx]\n",
        "\n",
        "# em_weights = model.wv\n",
        "# embedding = nn.Embedding.from_pretrained(em_weights)\n",
        "\n",
        "print(POIs)\n",
        "\n",
        "# max_sentence_length = 33\n",
        "# max_poi_length = 21\n",
        "# max_street_length = 16\n",
        "train_input = np.zeros([len(raw_addr_words_train), 64], dtype=np.int32)\n",
        "test_input = np.zeros([len(raw_addr_words_test), 64], dtype=np.int32)\n",
        "for i, sentence in enumerate(raw_addr_words_train):\n",
        "    for t, word in enumerate(sentence[:-1]):\n",
        "        train_input[i, t] = word2idx(word)\n",
        "for i, sentence in enumerate(raw_addr_words_test):\n",
        "    for t, word in enumerate(sentence[:-1]):\n",
        "        test_input[i, t] = word2idx(word)"
      ],
      "id": "boolean-antarctica",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Word2VecKeyedVectors' object has no attribute 'dim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-550ca1d437ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mem_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_addr_words_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m4.0000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m5.1000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6.3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;34m'Embeddings parameter is expected to be 2-dimensional'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'dim'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "returning-holiday"
      },
      "source": [
        "### 模型部分"
      ],
      "id": "returning-holiday"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sticky-messaging"
      },
      "source": [
        "torch.nn.LSTM(*args, **kwargs)\n",
        "- input_size – The number of expected features in the input x\n",
        "- hidden_size – The number of features in the hidden state h\n",
        "- num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
        "- bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "- batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
        "- dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
        "- bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
        "- proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
        "\n",
        "关于batch_first\n",
        "RNN的输入是(seq_len, batch_size, input_size)，batch_size位于第二维度\n",
        "https://www.jianshu.com/p/41c15d301542\n",
        "https://www.cnblogs.com/picassooo/p/13637140.html"
      ],
      "id": "sticky-messaging"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uniform-trigger"
      },
      "source": [
        "# Hyper Parameters\n",
        "EPOCH = 1               \n",
        "BATCH_SIZE = 32\n",
        "TIME_STEP = 66          # rnn time step / image height # 设置为max_sentence_length的两倍，应该够用\n",
        "INPUT_SIZE = 100         # rnn input size / image width # 和前面的vector的长度保持一致\n",
        "HIDDEN_SIZE = 100\n",
        "LR = 0.01"
      ],
      "id": "uniform-trigger",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "convinced-whale"
      },
      "source": [
        "import pytorch\n",
        "from torch import nn\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(         \n",
        "            input_size=INPUT_SIZE,\n",
        "            hidden_size=HIDDEN_SIZE,         # rnn hidden unit\n",
        "            num_layers=1,           # number of rnn layer\n",
        "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(100, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape (batch, time_step, input_size)\n",
        "        # r_out shape (batch, time_step, output_size)\n",
        "        # h_n shape (n_layers, batch, hidden_size)\n",
        "        # h_c shape (n_layers, batch, hidden_size)\n",
        "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
        "\n",
        "        # choose r_out at the last time step\n",
        "        out = self.out(r_out[:, -1, :])\n",
        "        return out"
      ],
      "id": "convinced-whale",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "third-winning"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255.   # shape (2000, 28, 28) value in range(0,1) 注意训练数据会自动规范化，但测试数据不会，所以这里要手动除以255，否则会导致训练不收敛\n",
        "test_y = test_data.test_labels.numpy()[:2000]    # covert to numpy array\n",
        "\n",
        "rnn = RNN()\n",
        "print(rnn)\n",
        "\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) \n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# training and testing\n",
        "for epoch in range(EPOCH):\n",
        "    for step, (b_x, b_y) in enumerate(train_loader):        # gives batch data\n",
        "        b_x = b_x.view(-1, INPUT_SIZE)              # reshape x to (batch, time_step, input_size)\n",
        "\n",
        "        output = rnn(b_x)                               # rnn output\n",
        "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
        "        optimizer.zero_grad()                           # clear gradients for this training step\n",
        "        loss.backward()                                 # backpropagation, compute gradients\n",
        "        optimizer.step()                                # apply gradients\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            test_output = rnn(test_x)                   # (samples, time_step, input_size)\n",
        "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
        "            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n",
        "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
        "\n",
        "# print 10 predictions from test data\n",
        "test_output = rnn(test_x[:10].view(-1, 28, 28))\n",
        "pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
        "print(pred_y, 'prediction number')\n",
        "print(test_y[:10], 'real number')"
      ],
      "id": "third-winning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "informational-sunset"
      },
      "source": [
        "### Encoder-Decoder translation模型\n",
        "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ],
      "id": "informational-sunset"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmFsbvTAIzy1",
        "outputId": "2e6e7f98-89a8-4478-897e-6f895a7488f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "DmFsbvTAIzy1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsCTUd0nJGLn"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/shopee contest2')"
      ],
      "id": "vsCTUd0nJGLn",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arbitrary-debate"
      },
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "arbitrary-debate",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pediatric-polish"
      },
      "source": [
        "#### 辅助代码"
      ],
      "id": "pediatric-polish"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grateful-italic"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "id": "grateful-italic",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generic-particle"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "id": "generic-particle",
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "configured-folder"
      },
      "source": [
        "#### 数据预处理"
      ],
      "id": "configured-folder"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "known-bubble"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "id": "known-bubble",
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "direct-lloyd"
      },
      "source": [
        "import pandas as pd\n",
        "def readLangs(lang_name):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    df_train = pd.read_csv(\"train.csv\")\n",
        "    raw_addr_train = df_train['raw_address'].tolist()\n",
        "    df_test = pd.read_csv(\"test.csv\")\n",
        "    raw_addr_test = df_test['raw_address'].tolist()\n",
        "    POIs = []\n",
        "    # streets = []\n",
        "    for gt in df_train['POI/street'].tolist():\n",
        "        POI = gt.split('/', 1)[0]\n",
        "        POIs.append(POI)\n",
        "        # street = gt.split('/', 1)[1]\n",
        "        # streets.append(street)\n",
        "\n",
        "    # Combine addresses and POIs into pairs\n",
        "    pairs = []\n",
        "    for i in range(len(raw_addr_train)):\n",
        "        pairs.append([raw_addr_train[i].replace(\",\", \"\"), POIs[i]])\n",
        "        # pairs.append([raw_addr_train[i].replace(\",\", \"\"), streets[i]])\n",
        "    \n",
        "    tests = []\n",
        "    for sent in raw_addr_test:\n",
        "        tests.append(sent.replace(\",\",\"\"))\n",
        "    \n",
        "    # Reverse pairs, make Lang instances\n",
        "    lang = Lang(lang_name)\n",
        "\n",
        "    return lang, pairs, tests"
      ],
      "id": "direct-lloyd",
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "musical-scott",
        "outputId": "e6496540-faec-4005-db11-bd05f9956f93"
      },
      "source": [
        "def prepareData(lang_name):\n",
        "    lang, pairs, tests = readLangs(lang_name)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Read %s testing sentence\" % len(tests))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        lang.addSentence(pair[0])\n",
        "        lang.addSentence(pair[1])\n",
        "    # 取前20000个作为验证集validation\n",
        "    validates = pairs[:20000]\n",
        "    pairs = pairs[20000:]\n",
        "    for test in tests:\n",
        "        lang.addSentence(test)\n",
        "    print(\"Counted words:\")\n",
        "    print(lang.name, lang.n_words)\n",
        "    return lang, pairs, validates, tests\n",
        "\n",
        "lang, pairs, validates, tests = prepareData('All_words')\n",
        "print(random.choice(pairs))"
      ],
      "id": "musical-scott",
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 300000 sentence pairs\n",
            "Read 50000 testing sentence\n",
            "Counting words...\n",
            "Counted words:\n",
            "All_words 120699\n",
            "['kuningan timur men anugrah 19th floor kan taman e 3.3 jl mega kuni rt 1 2 setia budi', 'menara anugrah 19th floor kantor taman e 3.3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arabic-athletics"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "id": "arabic-athletics",
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoeRWZLYJwHm"
      },
      "source": [
        "MAX_LENGTH = 33"
      ],
      "id": "DoeRWZLYJwHm",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "virgin-clearing"
      },
      "source": [
        "#### 核心代码"
      ],
      "id": "virgin-clearing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affecting-mercy"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden) # 上一个unit的output和hidden到这一个unit来\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "id": "affecting-mercy",
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worst-device"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "id": "worst-device",
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "physical-bench"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "id": "physical-bench",
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "southern-architecture"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "id": "southern-architecture",
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "composite-coordinator"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "id": "composite-coordinator",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "another-knitting"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=20):\n",
        "    for i in range(n):\n",
        "        # 取validates验证集来查看\n",
        "        pair = random.choice(validates)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "id": "another-knitting",
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffYxfLtaYUie"
      },
      "source": [
        "import pandas as pd\n",
        "def saveSentence(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(lang.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        output_sentence = ' '.join(decoded_words)\n",
        "        return output_sentence\n",
        "\n",
        "def saveTrainOutput(encoder, decoder, pairs, name):\n",
        "    output_sentences = []\n",
        "    for i, sent in enumerate(pairs):\n",
        "        if i%10000 == 0:\n",
        "            print(\"%d %d%%\" % (i, i*100/len(pairs)))\n",
        "        if name == 'POI':\n",
        "            output_sentences.append({'POI': saveSentence(encoder, decoder, sent[0])})\n",
        "        elif name == 'street':\n",
        "            output_sentences.append({'street': saveSentence(encoder, decoder, sent[0])})\n",
        "    df_output = pd.DataFrame(output_sentences)\n",
        "    if name == 'POI':\n",
        "        df_output.to_csv(\"saved_answers/train_POIs.csv\")\n",
        "    elif name == 'street':\n",
        "        df_output.to_csv(\"saved_answers/train_streets.csv\")\n",
        "\n",
        "def saveValOutput(encoder, decoder, validates, name):\n",
        "    output_sentences = []\n",
        "    for i, sent in enumerate(validates):\n",
        "        if i%5000 == 0:\n",
        "            print(\"%d %d%%\" % (i, i*100/len(validates)))\n",
        "        if name == 'POI':\n",
        "            output_sentences.append({'POI': saveSentence(encoder, decoder, sent[0])})\n",
        "        elif name == 'street':\n",
        "            output_sentences.append({'street': saveSentence(encoder, decoder, sent[0])})\n",
        "    df_output = pd.DataFrame(output_sentences)\n",
        "    if name == 'POI':\n",
        "        df_output.to_csv(\"saved_answers/val_POIs.csv\")\n",
        "    elif name == 'street':\n",
        "        df_output.to_csv(\"saved_answers/val_streets.csv\")\n",
        "\n",
        "def saveTestOutput(encoder, decoder, tests, name):\n",
        "    output_sentences = []\n",
        "    for i, sent in enumerate(tests):\n",
        "        if i%5000 == 0:\n",
        "            print(\"%d %d%%\" % (i, i*100/len(tests)))\n",
        "        if name == 'POI':\n",
        "            output_sentences.append({'POI': saveSentence(encoder, decoder, sent)})\n",
        "        elif name == 'street':\n",
        "            output_sentences.append({'street': saveSentence(encoder, decoder, sent)})\n",
        "    df_output = pd.DataFrame(output_sentences)\n",
        "    if name == 'POI':\n",
        "        df_output.to_csv(\"saved_answers/test_POIs.csv\")\n",
        "    elif name == 'street':\n",
        "        df_output.to_csv(\"saved_answers/test_streets.csv\")"
      ],
      "id": "ffYxfLtaYUie",
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8aBjsOdwfuM",
        "outputId": "84aee747-7758-41fe-fba9-d6da87ae3fe1"
      },
      "source": [
        "# continue-train\n",
        "n_iterations = 100000\n",
        "encoder1 = torch.load('saved_models/POI-encoder1-%diters.pt'%(300000-n_iterations))\n",
        "attn_decoder1 = torch.load('saved_models/POI-attn_decoder1-%diters.pt'%(300000-n_iterations))\n",
        "trainIters(encoder1, attn_decoder1, n_iterations, print_every=5000)\n",
        "torch.save(encoder1, 'saved_models/POI-encoder1-300000iters.pt')\n",
        "torch.save(attn_decoder1, 'saved_models/POI-attn_decoder1-300000iters.pt')"
      ],
      "id": "J8aBjsOdwfuM",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 49s (- 34m 49s) (5000 5%) 1.9025\n",
            "3m 35s (- 32m 16s) (10000 10%) 1.8816\n",
            "5m 20s (- 30m 17s) (15000 15%) 1.8796\n",
            "7m 5s (- 28m 20s) (20000 20%) 1.7657\n",
            "8m 50s (- 26m 30s) (25000 25%) 1.8195\n",
            "10m 35s (- 24m 42s) (30000 30%) 1.8460\n",
            "12m 19s (- 22m 53s) (35000 35%) 1.8601\n",
            "14m 4s (- 21m 6s) (40000 40%) 1.8202\n",
            "15m 48s (- 19m 19s) (45000 45%) 1.7702\n",
            "17m 34s (- 17m 34s) (50000 50%) 1.7759\n",
            "19m 19s (- 15m 48s) (55000 55%) 1.7290\n",
            "21m 5s (- 14m 3s) (60000 60%) 1.8111\n",
            "22m 50s (- 12m 17s) (65000 65%) 1.7963\n",
            "24m 35s (- 10m 32s) (70000 70%) 1.7362\n",
            "26m 21s (- 8m 47s) (75000 75%) 1.7547\n",
            "28m 6s (- 7m 1s) (80000 80%) 1.7239\n",
            "29m 51s (- 5m 16s) (85000 85%) 1.7196\n",
            "31m 37s (- 3m 30s) (90000 90%) 1.6645\n",
            "33m 23s (- 1m 45s) (95000 95%) 1.7495\n",
            "35m 8s (- 0m 0s) (100000 100%) 1.7176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "impressed-desire",
        "outputId": "4d7e78ea-e36c-4c94-ff3f-54bc877c112f"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "n_iterations = 400000\n",
        "trainIters(encoder1, attn_decoder1, n_iterations, print_every=5000)\n",
        "torch.save(encoder1, 'saved_models/POI-encoder1-%diters.pt'%n_iterations)\n",
        "torch.save(attn_decoder1, 'saved_models/POI-attn_decoder1-%diters.pt'%n_iterations)"
      ],
      "id": "impressed-desire",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 2s (- 161m 8s) (5000 1%) 3.2216\n",
            "3m 44s (- 145m 54s) (10000 2%) 2.8536\n",
            "5m 26s (- 139m 50s) (15000 3%) 2.8211\n",
            "7m 9s (- 136m 5s) (20000 5%) 2.6845\n",
            "8m 52s (- 133m 7s) (25000 6%) 2.6744\n",
            "10m 35s (- 130m 38s) (30000 7%) 2.6410\n",
            "12m 18s (- 128m 21s) (35000 8%) 2.5117\n",
            "14m 1s (- 126m 16s) (40000 10%) 2.5091\n",
            "15m 45s (- 124m 20s) (45000 11%) 2.5061\n",
            "17m 28s (- 122m 21s) (50000 12%) 2.4185\n",
            "19m 12s (- 120m 28s) (55000 13%) 2.3698\n",
            "20m 55s (- 118m 36s) (60000 15%) 2.3372\n",
            "22m 39s (- 116m 44s) (65000 16%) 2.3538\n",
            "24m 22s (- 114m 54s) (70000 17%) 2.2624\n",
            "26m 6s (- 113m 6s) (75000 18%) 2.2959\n",
            "27m 50s (- 111m 20s) (80000 20%) 2.3513\n",
            "29m 34s (- 109m 36s) (85000 21%) 2.2181\n",
            "31m 18s (- 107m 50s) (90000 22%) 2.1396\n",
            "33m 2s (- 106m 5s) (95000 23%) 2.1507\n",
            "34m 46s (- 104m 20s) (100000 25%) 2.1505\n",
            "36m 30s (- 102m 33s) (105000 26%) 2.1149\n",
            "38m 14s (- 100m 49s) (110000 27%) 2.0905\n",
            "39m 58s (- 99m 5s) (115000 28%) 2.1121\n",
            "41m 43s (- 97m 21s) (120000 30%) 2.0849\n",
            "43m 28s (- 95m 38s) (125000 31%) 2.1054\n",
            "45m 13s (- 93m 55s) (130000 32%) 2.0660\n",
            "46m 57s (- 92m 11s) (135000 33%) 2.0215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYMwWlK5QwsG",
        "outputId": "7dbd488e-1654-4e14-fcc2-9fd5895010b2"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "n_iterations = 400000\n",
        "trainIters(encoder1, attn_decoder1, n_iterations, print_every=5000)\n",
        "torch.save(encoder1, 'saved_models/street-encoder1-%diters.pt'%n_iterations)\n",
        "torch.save(attn_decoder1, 'saved_models/street-attn_decoder1-%diters.pt'%n_iterations)"
      ],
      "id": "dYMwWlK5QwsG",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 53s (- 112m 3s) (5000 1%) 4.9881\n",
            "3m 32s (- 102m 42s) (10000 3%) 4.4488\n",
            "5m 11s (- 98m 43s) (15000 5%) 4.2210\n",
            "6m 50s (- 95m 51s) (20000 6%) 4.0978\n",
            "8m 30s (- 93m 37s) (25000 8%) 3.9365\n",
            "10m 11s (- 91m 41s) (30000 10%) 3.7642\n",
            "11m 51s (- 89m 50s) (35000 11%) 3.6750\n",
            "13m 32s (- 88m 1s) (40000 13%) 3.4709\n",
            "15m 14s (- 86m 23s) (45000 15%) 3.3909\n",
            "16m 56s (- 84m 40s) (50000 16%) 3.2648\n",
            "18m 38s (- 83m 0s) (55000 18%) 3.1567\n",
            "20m 20s (- 81m 21s) (60000 20%) 3.1587\n",
            "22m 2s (- 79m 40s) (65000 21%) 3.0064\n",
            "23m 44s (- 78m 0s) (70000 23%) 2.9263\n",
            "25m 27s (- 76m 21s) (75000 25%) 2.8871\n",
            "27m 8s (- 74m 38s) (80000 26%) 2.7798\n",
            "28m 50s (- 72m 56s) (85000 28%) 2.7262\n",
            "30m 32s (- 71m 16s) (90000 30%) 2.6267\n",
            "32m 15s (- 69m 35s) (95000 31%) 2.6481\n",
            "33m 57s (- 67m 54s) (100000 33%) 2.5631\n",
            "35m 39s (- 66m 13s) (105000 35%) 2.5133\n",
            "37m 22s (- 64m 33s) (110000 36%) 2.4839\n",
            "39m 4s (- 62m 52s) (115000 38%) 2.4444\n",
            "40m 47s (- 61m 11s) (120000 40%) 2.3950\n",
            "42m 30s (- 59m 30s) (125000 41%) 2.3498\n",
            "44m 13s (- 57m 49s) (130000 43%) 2.3253\n",
            "45m 55s (- 56m 8s) (135000 45%) 2.2700\n",
            "47m 38s (- 54m 26s) (140000 46%) 2.2355\n",
            "49m 20s (- 52m 44s) (145000 48%) 2.2133\n",
            "51m 3s (- 51m 3s) (150000 50%) 2.2326\n",
            "52m 46s (- 49m 22s) (155000 51%) 2.2061\n",
            "54m 29s (- 47m 40s) (160000 53%) 2.1179\n",
            "56m 12s (- 45m 59s) (165000 55%) 2.1221\n",
            "57m 55s (- 44m 17s) (170000 56%) 2.0752\n",
            "59m 38s (- 42m 36s) (175000 58%) 2.0277\n",
            "61m 21s (- 40m 54s) (180000 60%) 2.0312\n",
            "63m 5s (- 39m 12s) (185000 61%) 2.0238\n",
            "64m 48s (- 37m 31s) (190000 63%) 2.0118\n",
            "66m 32s (- 35m 49s) (195000 65%) 1.9753\n",
            "68m 15s (- 34m 7s) (200000 66%) 1.9165\n",
            "69m 58s (- 32m 25s) (205000 68%) 1.8755\n",
            "71m 42s (- 30m 43s) (210000 70%) 1.8946\n",
            "73m 26s (- 29m 1s) (215000 71%) 1.8977\n",
            "75m 10s (- 27m 20s) (220000 73%) 1.8594\n",
            "76m 53s (- 25m 37s) (225000 75%) 1.8915\n",
            "78m 38s (- 23m 55s) (230000 76%) 1.7944\n",
            "80m 22s (- 22m 13s) (235000 78%) 1.8839\n",
            "82m 7s (- 20m 31s) (240000 80%) 1.8225\n",
            "83m 51s (- 18m 49s) (245000 81%) 1.7966\n",
            "85m 34s (- 17m 6s) (250000 83%) 1.7923\n",
            "87m 18s (- 15m 24s) (255000 85%) 1.7985\n",
            "89m 2s (- 13m 41s) (260000 86%) 1.7749\n",
            "90m 47s (- 11m 59s) (265000 88%) 1.7435\n",
            "92m 31s (- 10m 16s) (270000 90%) 1.7285\n",
            "94m 16s (- 8m 34s) (275000 91%) 1.7680\n",
            "96m 0s (- 6m 51s) (280000 93%) 1.7061\n",
            "97m 44s (- 5m 8s) (285000 95%) 1.6497\n",
            "99m 28s (- 3m 25s) (290000 96%) 1.6846\n",
            "101m 12s (- 1m 42s) (295000 98%) 1.6756\n",
            "102m 56s (- 0m 0s) (300000 100%) 1.6532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cX3yQOBbQTo",
        "outputId": "25d0e45e-c93f-4341-c79c-c68efeb0bd68"
      },
      "source": [
        "n_iterations = 200000\n",
        "encoder1 = torch.load('saved_models/POI-encoder1-%diters.pt'%n_iterations)\n",
        "attn_decoder1 = torch.load('saved_models/POI-attn_decoder1-%diters.pt'%n_iterations)\n",
        "# saveTrainOutput(encoder1, attn_decoder1, pairs[:1000], 'POI')\n",
        "saveTestOutput(encoder1, attn_decoder1, tests, 'POI')"
      ],
      "id": "2cX3yQOBbQTo",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0%\n",
            "5000 10%\n",
            "10000 20%\n",
            "15000 30%\n",
            "20000 40%\n",
            "25000 50%\n",
            "30000 60%\n",
            "35000 70%\n",
            "40000 80%\n",
            "45000 90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA3I00YJ9xnM",
        "outputId": "85eab013-fc6c-4803-ac00-404b79c0084a"
      },
      "source": [
        "n_iterations = 300000\n",
        "encoder1 = torch.load('saved_models/street-encoder1-%diters.pt'%n_iterations)\n",
        "attn_decoder1 = torch.load('saved_models/street-attn_decoder1-%diters.pt'%n_iterations)\n",
        "# saveTrainOutput(encoder1, attn_decoder1, pairs[:1000], 'street')\n",
        "saveTestOutput(encoder1, attn_decoder1, tests, 'street')"
      ],
      "id": "TA3I00YJ9xnM",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0%\n",
            "5000 10%\n",
            "10000 20%\n",
            "15000 30%\n",
            "20000 40%\n",
            "25000 50%\n",
            "30000 60%\n",
            "35000 70%\n",
            "40000 80%\n",
            "45000 90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8wOmt9Z5glV"
      },
      "source": [
        "# 存储validates和tests的结果\n",
        "n_iterations = 400000\n",
        "encoder1 = torch.load('saved_models/POI-encoder1-%diters.pt'%n_iterations)\n",
        "attn_decoder1 = torch.load('saved_models/POI-attn_decoder1-%diters.pt'%n_iterations)\n",
        "saveValOutput(encoder1, attn_decoder1, validates, 'POI')\n",
        "saveTestOutput(encoder1, attn_decoder1, tests, 'POI')\n",
        "encoder1 = torch.load('saved_models/street-encoder1-%diters.pt'%n_iterations)\n",
        "attn_decoder1 = torch.load('saved_models/street-attn_decoder1-%diters.pt'%n_iterations)\n",
        "saveValOutput(encoder1, attn_decoder1, validates, 'street')\n",
        "saveTestOutput(encoder1, attn_decoder1, tests, 'street')"
      ],
      "id": "q8wOmt9Z5glV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spare-overall",
        "outputId": "e32e057c-5b58-451a-8035-5dd9a93cab66"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "id": "spare-overall",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> bagus advert raya batu bulan 61 sukawati\n",
            "= raya batu bulan\n",
            "< raya batu <EOS>\n",
            "\n",
            "> rumah tumbuh blok t nomer 5 bapak romza aidi dekat taman ribang gale\n",
            "= \n",
            "<  <EOS>\n",
            "\n",
            "> bumi alam pertiwi cv balow 2\n",
            "= balowerti 2\n",
            "<  <EOS>\n",
            "\n",
            "> war leo raya purwo purwodadi\n",
            "= raya purwo\n",
            "< raya purwo <EOS>\n",
            "\n",
            "> seja 88-104 cimekar cileunyi\n",
            "= seja\n",
            "<  <EOS>\n",
            "\n",
            "> toko fatih raya lab karaton\n",
            "= raya labuan\n",
            "< raya lab <EOS>\n",
            "\n",
            "> g h. timan no 101 ceger rt 5 5 cipayung\n",
            "= g h. timan\n",
            "< h. h. <EOS>\n",
            "\n",
            "> lubang buaya gg. h. jirin 10 rt 5 7 cipayung\n",
            "= \n",
            "< gg. h. h. <EOS>\n",
            "\n",
            "> kiara sari vii margasari kel.\n",
            "= kiara sari vii\n",
            "< kiara sari vii <EOS>\n",
            "\n",
            "> jl tera 89a medan estate percut sei tuan\n",
            "= jl tera\n",
            "< jl tera <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}