{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01098,
     "end_time": "2021-03-14T11:39:39.344488",
     "exception": false,
     "start_time": "2021-03-14T11:39:39.333508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For sharing and learning. \n",
    "\n",
    "Seq2Seq. The intuition here is that since we need to map the short form of a word to its long form. Instead of mapping them manually, maybe the neural network could learn the mapping and generate the correct POI and street names. \n",
    "\n",
    "Didn't get to make seq2seq work. \n",
    "\n",
    "Reference:\n",
    "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:39.371106Z",
     "iopub.status.busy": "2021-03-14T11:39:39.370413Z",
     "iopub.status.idle": "2021-03-14T11:39:45.303276Z",
     "shell.execute_reply": "2021-03-14T11:39:45.302530Z"
    },
    "papermill": {
     "duration": 5.949002,
     "end_time": "2021-03-14T11:39:45.303499",
     "exception": false,
     "start_time": "2021-03-14T11:39:39.354497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:45.348302Z",
     "iopub.status.busy": "2021-03-14T11:39:45.343917Z",
     "iopub.status.idle": "2021-03-14T11:39:45.350919Z",
     "shell.execute_reply": "2021-03-14T11:39:45.350487Z"
    },
    "papermill": {
     "duration": 0.035808,
     "end_time": "2021-03-14T11:39:45.351027",
     "exception": false,
     "start_time": "2021-03-14T11:39:45.315219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import attention.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:45.381591Z",
     "iopub.status.busy": "2021-03-14T11:39:45.381041Z",
     "iopub.status.idle": "2021-03-14T11:39:46.180504Z",
     "shell.execute_reply": "2021-03-14T11:39:46.181087Z"
    },
    "papermill": {
     "duration": 0.819911,
     "end_time": "2021-03-14T11:39:46.181302",
     "exception": false,
     "start_time": "2021-03-14T11:39:45.361391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/scl-2021-ds/train.csv\")\n",
    "test = pd.read_csv(\"../input/scl-2021-ds/test.csv\")\n",
    "\n",
    "print(len(df))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:46.241026Z",
     "iopub.status.busy": "2021-03-14T11:39:46.240222Z",
     "iopub.status.idle": "2021-03-14T11:39:48.074122Z",
     "shell.execute_reply": "2021-03-14T11:39:48.074523Z"
    },
    "papermill": {
     "duration": 1.881317,
     "end_time": "2021-03-14T11:39:48.074668",
     "exception": false,
     "start_time": "2021-03-14T11:39:46.193351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_address</th>\n",
       "      <th>POI/street</th>\n",
       "      <th>POI</th>\n",
       "      <th>street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39691</th>\n",
       "      <td>39691</td>\n",
       "      <td>wisma ratu 3, jatimakmur pondok gede</td>\n",
       "      <td>/</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229553</th>\n",
       "      <td>229553</td>\n",
       "      <td>raden inten ii 108 duren sawit rt 2 7 duren sawit</td>\n",
       "      <td>/raden inten ii</td>\n",
       "      <td></td>\n",
       "      <td>raden inten ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284545</th>\n",
       "      <td>284545</td>\n",
       "      <td>citilink, raya darmo per 3 dukuh pakis</td>\n",
       "      <td>citilink/raya darmo per 3</td>\n",
       "      <td>citilink</td>\n",
       "      <td>raya darmo per 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184262</th>\n",
       "      <td>184262</td>\n",
       "      <td>war burung gor kru luas-b sumut subulussalam selatan</td>\n",
       "      <td>warung burung goreng/kru luas-b sumut</td>\n",
       "      <td>warung burung goreng</td>\n",
       "      <td>kru luas-b sumut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24867</th>\n",
       "      <td>24867</td>\n",
       "      <td>ciantra cire 4, 17530 cikarang selatan</td>\n",
       "      <td>/cire 4</td>\n",
       "      <td></td>\n",
       "      <td>cire 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147785</th>\n",
       "      <td>147785</td>\n",
       "      <td>kayu putih wari 3 no 21 rt 1 8 pulo gadung</td>\n",
       "      <td>/wari 3</td>\n",
       "      <td></td>\n",
       "      <td>wari 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139696</th>\n",
       "      <td>139696</td>\n",
       "      <td>sma raden fatah cimangu sala cimanggu</td>\n",
       "      <td>sma raden fatah cimangu/salangk</td>\n",
       "      <td>sma raden fatah cimangu</td>\n",
       "      <td>salangk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205345</th>\n",
       "      <td>205345</td>\n",
       "      <td>pati 212 tanjung paku tanjung harapan</td>\n",
       "      <td>/pati</td>\n",
       "      <td></td>\n",
       "      <td>pati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92008</th>\n",
       "      <td>92008</td>\n",
       "      <td>masjid al-hikmah, suka sari</td>\n",
       "      <td>masjid al-hikmah/</td>\n",
       "      <td>masjid al-hikmah</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160504</th>\n",
       "      <td>160504</td>\n",
       "      <td>gg. ikh 1 cijantung pasar rebo</td>\n",
       "      <td>/</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           raw_address  \\\n",
       "39691    39691                  wisma ratu 3, jatimakmur pondok gede   \n",
       "229553  229553     raden inten ii 108 duren sawit rt 2 7 duren sawit   \n",
       "284545  284545                citilink, raya darmo per 3 dukuh pakis   \n",
       "184262  184262  war burung gor kru luas-b sumut subulussalam selatan   \n",
       "24867    24867                ciantra cire 4, 17530 cikarang selatan   \n",
       "147785  147785            kayu putih wari 3 no 21 rt 1 8 pulo gadung   \n",
       "139696  139696                 sma raden fatah cimangu sala cimanggu   \n",
       "205345  205345                 pati 212 tanjung paku tanjung harapan   \n",
       "92008    92008                           masjid al-hikmah, suka sari   \n",
       "160504  160504                        gg. ikh 1 cijantung pasar rebo   \n",
       "\n",
       "                                   POI/street                      POI  \\\n",
       "39691                                       /                            \n",
       "229553                        /raden inten ii                            \n",
       "284545              citilink/raya darmo per 3                 citilink   \n",
       "184262  warung burung goreng/kru luas-b sumut     warung burung goreng   \n",
       "24867                                 /cire 4                            \n",
       "147785                                /wari 3                            \n",
       "139696        sma raden fatah cimangu/salangk  sma raden fatah cimangu   \n",
       "205345                                  /pati                            \n",
       "92008                       masjid al-hikmah/         masjid al-hikmah   \n",
       "160504                                      /                            \n",
       "\n",
       "                  street  \n",
       "39691                     \n",
       "229553    raden inten ii  \n",
       "284545  raya darmo per 3  \n",
       "184262  kru luas-b sumut  \n",
       "24867             cire 4  \n",
       "147785            wari 3  \n",
       "139696           salangk  \n",
       "205345              pati  \n",
       "92008                     \n",
       "160504                    "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "df['POI'] = df['POI/street'].str.extract(r'(.*)/', expand=True)\n",
    "df['street'] = df['POI/street'].str.extract(r'/(.*)', expand=True)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:48.138073Z",
     "iopub.status.busy": "2021-03-14T11:39:48.137243Z",
     "iopub.status.idle": "2021-03-14T11:39:48.270962Z",
     "shell.execute_reply": "2021-03-14T11:39:48.271419Z"
    },
    "papermill": {
     "duration": 0.185331,
     "end_time": "2021-03-14T11:39:48.271561",
     "exception": false,
     "start_time": "2021-03-14T11:39:48.086230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_address</th>\n",
       "      <th>POI/street</th>\n",
       "      <th>POI</th>\n",
       "      <th>street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110119</th>\n",
       "      <td>110119</td>\n",
       "      <td>siak no 65 atm bri, siantar utara</td>\n",
       "      <td>atm bri/siak</td>\n",
       "      <td>_START_ atm bri _END_</td>\n",
       "      <td>_START_ siak _END_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224057</th>\n",
       "      <td>224057</td>\n",
       "      <td>masjid jami ala kela, kel gad, kelapa gading timur</td>\n",
       "      <td>masjid jami alamin kela/kel gad</td>\n",
       "      <td>_START_ masjid jami alamin kela _END_</td>\n",
       "      <td>_START_ kel gad _END_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15179</th>\n",
       "      <td>15179</td>\n",
       "      <td>kantor keuchik pulo ie, 23771 tangse</td>\n",
       "      <td>kantor keuchik pulo ie/</td>\n",
       "      <td>_START_ kantor keuchik pulo ie _END_</td>\n",
       "      <td>_START_  _END_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173769</th>\n",
       "      <td>173769</td>\n",
       "      <td>smp negeri 1 aere,</td>\n",
       "      <td>smp negeri 1 aere/</td>\n",
       "      <td>_START_ smp negeri 1 aere _END_</td>\n",
       "      <td>_START_  _END_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79536</th>\n",
       "      <td>79536</td>\n",
       "      <td>kebon agung, toko bang mriyan agung, margo mulyo</td>\n",
       "      <td>toko bangunan mriyan agung/kebon agung</td>\n",
       "      <td>_START_ toko bangunan mriyan agung _END_</td>\n",
       "      <td>_START_ kebon agung _END_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                         raw_address  \\\n",
       "110119  110119                   siak no 65 atm bri, siantar utara   \n",
       "224057  224057  masjid jami ala kela, kel gad, kelapa gading timur   \n",
       "15179    15179                kantor keuchik pulo ie, 23771 tangse   \n",
       "173769  173769                                  smp negeri 1 aere,   \n",
       "79536    79536    kebon agung, toko bang mriyan agung, margo mulyo   \n",
       "\n",
       "                                    POI/street  \\\n",
       "110119                            atm bri/siak   \n",
       "224057         masjid jami alamin kela/kel gad   \n",
       "15179                  kantor keuchik pulo ie/   \n",
       "173769                      smp negeri 1 aere/   \n",
       "79536   toko bangunan mriyan agung/kebon agung   \n",
       "\n",
       "                                             POI                     street  \n",
       "110119                     _START_ atm bri _END_         _START_ siak _END_  \n",
       "224057     _START_ masjid jami alamin kela _END_      _START_ kel gad _END_  \n",
       "15179       _START_ kantor keuchik pulo ie _END_             _START_  _END_  \n",
       "173769           _START_ smp negeri 1 aere _END_             _START_  _END_  \n",
       "79536   _START_ toko bangunan mriyan agung _END_  _START_ kebon agung _END_  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out all POI = null\n",
    "df = df[df['POI'] != '']\n",
    "\n",
    "\n",
    "df['POI'] = df['POI'].apply(lambda x : '_START_ '+ x + ' _END_')\n",
    "df['street'] = df['street'].apply(lambda x : '_START_ '+ x + ' _END_')\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:48.300924Z",
     "iopub.status.busy": "2021-03-14T11:39:48.300119Z",
     "iopub.status.idle": "2021-03-14T11:39:48.318173Z",
     "shell.execute_reply": "2021-03-14T11:39:48.318708Z"
    },
    "papermill": {
     "duration": 0.035042,
     "end_time": "2021-03-14T11:39:48.318890",
     "exception": false,
     "start_time": "2021-03-14T11:39:48.283848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109540 11951\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.9\n",
    "\n",
    "train = df[msk]\n",
    "val = df[~msk]\n",
    "\n",
    "x_tr = train['raw_address']\n",
    "y1_tr = train['POI']\n",
    "y2_tr = train['street']\n",
    "\n",
    "x_val = val['raw_address']\n",
    "y1_val = val['POI']\n",
    "y2_val = val['street']\n",
    "\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:48.380456Z",
     "iopub.status.busy": "2021-03-14T11:39:48.375377Z",
     "iopub.status.idle": "2021-03-14T11:39:52.045721Z",
     "shell.execute_reply": "2021-03-14T11:39:52.045230Z"
    },
    "papermill": {
     "duration": 3.71378,
     "end_time": "2021-03-14T11:39:52.045852",
     "exception": false,
     "start_time": "2021-03-14T11:39:48.332072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len_text=20 \n",
    "max_len_summary=5\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:52.114137Z",
     "iopub.status.busy": "2021-03-14T11:39:52.098792Z",
     "iopub.status.idle": "2021-03-14T11:39:55.596763Z",
     "shell.execute_reply": "2021-03-14T11:39:55.597298Z"
    },
    "papermill": {
     "duration": 3.5389,
     "end_time": "2021-03-14T11:39:55.597507",
     "exception": false,
     "start_time": "2021-03-14T11:39:52.058607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preparing a tokenizer for summary on training data \n",
    "y1_tokenizer = Tokenizer()\n",
    "y1_tokenizer.fit_on_texts(list(y1_tr))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y1_tr    =   y1_tokenizer.texts_to_sequences(y1_tr) \n",
    "y1_val   =   y1_tokenizer.texts_to_sequences(y1_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y1_tr    =   pad_sequences(y1_tr, maxlen=max_len_summary, padding='post')\n",
    "y1_val   =   pad_sequences(y1_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y1_voc_size  =   len(y1_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:39:55.647003Z",
     "iopub.status.busy": "2021-03-14T11:39:55.646357Z",
     "iopub.status.idle": "2021-03-14T11:39:59.966734Z",
     "shell.execute_reply": "2021-03-14T11:39:59.966171Z"
    },
    "papermill": {
     "duration": 4.350169,
     "end_time": "2021-03-14T11:39:59.966873",
     "exception": false,
     "start_time": "2021-03-14T11:39:55.616704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 100)      6298000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 20, 100), (N 80400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 20, 100), (N 80400       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    4667200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 20, 100), (N 80400       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 100),  80400       embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 100),  20100       lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 200)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 46672)  9381072     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 20,687,972\n",
      "Trainable params: 20,687,972\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session() \n",
    "latent_dim = 100 #500 \n",
    "\n",
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_len_text,)) \n",
    "enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True, return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Set up the decoder. \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y1_voc_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y1_voc_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:40:00.005328Z",
     "iopub.status.busy": "2021-03-14T11:40:00.002698Z",
     "iopub.status.idle": "2021-03-14T11:40:00.008752Z",
     "shell.execute_reply": "2021-03-14T11:40:00.009169Z"
    },
    "papermill": {
     "duration": 0.02918,
     "end_time": "2021-03-14T11:40:00.009304",
     "exception": false,
     "start_time": "2021-03-14T11:39:59.980124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:40:00.040162Z",
     "iopub.status.busy": "2021-03-14T11:40:00.039488Z",
     "iopub.status.idle": "2021-03-14T11:40:00.042369Z",
     "shell.execute_reply": "2021-03-14T11:40:00.041953Z"
    },
    "papermill": {
     "duration": 0.019354,
     "end_time": "2021-03-14T11:40:00.042476",
     "exception": false,
     "start_time": "2021-03-14T11:40:00.023122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:40:00.074355Z",
     "iopub.status.busy": "2021-03-14T11:40:00.073859Z",
     "iopub.status.idle": "2021-03-14T11:44:10.183752Z",
     "shell.execute_reply": "2021-03-14T11:44:10.182848Z"
    },
    "papermill": {
     "duration": 250.128452,
     "end_time": "2021-03-14T11:44:10.183882",
     "exception": false,
     "start_time": "2021-03-14T11:40:00.055430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "214/214 [==============================] - 34s 120ms/step - loss: 6.6977 - val_loss: 4.7580\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 5.2328 - val_loss: 4.4194\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 24s 113ms/step - loss: 4.8451 - val_loss: 4.1709\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 4.5546 - val_loss: 3.9400\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 4.3248 - val_loss: 3.7807\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 24s 113ms/step - loss: 4.1172 - val_loss: 3.6366\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 24s 111ms/step - loss: 3.9509 - val_loss: 3.5875\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 3.8004 - val_loss: 3.4718\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 24s 111ms/step - loss: 3.6737 - val_loss: 3.3752\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 24s 113ms/step - loss: 3.5416 - val_loss: 3.2981\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "\n",
    "history=model.fit(x=[x_tr, y1_tr[:,:-1]], \n",
    "                  y=y1_tr.reshape(y1_tr.shape[0], y1_tr.shape[1], 1)[:,1:], \n",
    "                  epochs=epoch,\n",
    "                  callbacks=[es],\n",
    "                  batch_size=512, \n",
    "                  validation_data=([x_val,y1_val[:,:-1]], y1_val.reshape(y1_val.shape[0],y1_val.shape[1], 1)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:44:11.304622Z",
     "iopub.status.busy": "2021-03-14T11:44:11.303355Z",
     "iopub.status.idle": "2021-03-14T11:44:11.305889Z",
     "shell.execute_reply": "2021-03-14T11:44:11.306306Z"
    },
    "papermill": {
     "duration": 0.577949,
     "end_time": "2021-03-14T11:44:11.306451",
     "exception": false,
     "start_time": "2021-03-14T11:44:10.728502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y1_tokenizer.index_word \n",
    "reverse_source_word_index=x_tokenizer.index_word \n",
    "target_word_index=y1_tokenizer.word_index\n",
    "\n",
    "reverse_target_word_index[0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:44:12.414701Z",
     "iopub.status.busy": "2021-03-14T11:44:12.413903Z",
     "iopub.status.idle": "2021-03-14T11:44:12.685311Z",
     "shell.execute_reply": "2021-03-14T11:44:12.684867Z"
    },
    "papermill": {
     "duration": 0.823858,
     "end_time": "2021-03-14T11:44:12.685433",
     "exception": false,
     "start_time": "2021-03-14T11:44:11.861575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder inference\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:44:13.786569Z",
     "iopub.status.busy": "2021-03-14T11:44:13.785832Z",
     "iopub.status.idle": "2021-03-14T11:44:13.789688Z",
     "shell.execute_reply": "2021-03-14T11:44:13.789274Z"
    },
    "papermill": {
     "duration": 0.556047,
     "end_time": "2021-03-14T11:44:13.789813",
     "exception": false,
     "start_time": "2021-03-14T11:44:13.233766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'end'  or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:44:14.884083Z",
     "iopub.status.busy": "2021-03-14T11:44:14.882339Z",
     "iopub.status.idle": "2021-03-14T11:44:14.884652Z",
     "shell.execute_reply": "2021-03-14T11:44:14.885053Z"
    },
    "papermill": {
     "duration": 0.552085,
     "end_time": "2021-03-14T11:44:14.885196",
     "exception": false,
     "start_time": "2021-03-14T11:44:14.333111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T11:44:15.984112Z",
     "iopub.status.busy": "2021-03-14T11:44:15.983137Z",
     "iopub.status.idle": "2021-03-14T11:44:24.983501Z",
     "shell.execute_reply": "2021-03-14T11:44:24.984007Z"
    },
    "papermill": {
     "duration": 9.551533,
     "end_time": "2021-03-14T11:44:24.984220",
     "exception": false,
     "start_time": "2021-03-14T11:44:15.432687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: habitat terrace residence blok d8 jl masjid ciater ciater serpong 15317 \n",
      "Original POI: habitat terrace residence \n",
      "Predicted POI:  perumahan residence\n",
      "\n",
      "\n",
      "Review: batagor kuah ku pahl revo pondok bambu \n",
      "Original POI: batagor kuah ku \n",
      "Predicted POI:  kafe\n",
      "\n",
      "\n",
      "Review: tk islam daruss \n",
      "Original POI: tk islam darussalam \n",
      "Predicted POI:  tk al hidayah\n",
      "\n",
      "\n",
      "Review: raya teja pak tejakula \n",
      "Original POI: pak \n",
      "Predicted POI:  kopi pak\n",
      "\n",
      "\n",
      "Review: stasiun bekasi kota \n",
      "Original POI: stasiun bekasi \n",
      "Predicted POI:  stasiun bekasi\n",
      "\n",
      "\n",
      "Review: jalan tipar cakung no 26 depan rusun albo garasi \n",
      "Original POI: rusun albo \n",
      "Predicted POI:  rusun pesakih\n",
      "\n",
      "\n",
      "Review: jl terusan buah batu no 185 samping indomaret bandung \n",
      "Original POI: samping indomaret \n",
      "Predicted POI:  pasar rumput\n",
      "\n",
      "\n",
      "Review: alun alun kuli nusan kar \n",
      "Original POI: alun alun kuliner nusantara \n",
      "Predicted POI:  jembatan pelabuhan\n",
      "\n",
      "\n",
      "Review: cahaya abadi raya cang \n",
      "Original POI: cahaya abadi \n",
      "Predicted POI:  sinar abadi\n",
      "\n",
      "\n",
      "Review: duren tiga graha mamp 2nd floor raya mamp prap 100 rt 2 1 pancoran \n",
      "Original POI: mampang 2nd floor raya \n",
      "Predicted POI:  komplek puri\n",
      "\n",
      "\n",
      "Review: arj no 14 jasa wirobrajan wirobrajan \n",
      "Original POI: jasa \n",
      "Predicted POI:  klinik pln\n",
      "\n",
      "\n",
      "Review: rumah cuci ria cemp besar mawar banjarmasin tengah \n",
      "Original POI: rumah cuci rianda \n",
      "Predicted POI:  bengkel las\n",
      "\n",
      "\n",
      "Review: toko \n",
      "Original POI: toko \n",
      "Predicted POI:  toko\n",
      "\n",
      "\n",
      "Review: simo tamb 13 store 5 simomulyo sukomanunggal \n",
      "Original POI: store \n",
      "Predicted POI:  store\n",
      "\n",
      "\n",
      "Review: al barokah serv motor stas ker api \n",
      "Original POI: al barokah service motor \n",
      "Predicted POI:  jaya motor\n",
      "\n",
      "\n",
      "Review: m nasir ogan \n",
      "Original POI: m nasir \n",
      "Predicted POI:  pasar\n",
      "\n",
      "\n",
      "Review: kp warung kiara rt rw 001 005 ds mesjid peryayi kec kasemen \n",
      "Original POI: warung kiara \n",
      "Predicted POI:  warung kopi\n",
      "\n",
      "\n",
      "Review: basuk ayu logam kastolani balaradin \n",
      "Original POI: ayu logam kastolani \n",
      "Predicted POI:  cahaya teknik\n",
      "\n",
      "\n",
      "Review: sd neg i maysend h soem \n",
      "Original POI: sd negeri i \n",
      "Predicted POI:  sd negeri 1\n",
      "\n",
      "\n",
      "Review: depan simpang bandara bandara syekh hamzah fansyuri airport \n",
      "Original POI: syekh hamzah fansyuri airport \n",
      "Predicted POI:  gardenia park\n",
      "\n",
      "\n",
      "Review: toko sol moch toha muka \n",
      "Original POI: toko soleha \n",
      "Predicted POI:  toko\n",
      "\n",
      "\n",
      "Review: ika salon cilangkap cipayung \n",
      "Original POI: ika salon \n",
      "Predicted POI:  salon\n",
      "\n",
      "\n",
      "Review: indo tek raya indu pasirsari cikarang selatan \n",
      "Original POI: indo teknik \n",
      "Predicted POI:  cipta elektronik\n",
      "\n",
      "\n",
      "Review: sug pran upt mojogedang \n",
      "Original POI: upt mojogedang \n",
      "Predicted POI:  komp\n",
      "\n",
      "\n",
      "Review: toko gar sakti \n",
      "Original POI: toko \n",
      "Predicted POI:  toko obat\n",
      "\n",
      "\n",
      "Review: btn kolam blok l 39 doyo baru \n",
      "Original POI: btn kolam \n",
      "Predicted POI:  btn cabalu\n",
      "\n",
      "\n",
      "Review: tk kesaya mutiara blang kolak i bebesen \n",
      "Original POI: tk mutiara \n",
      "Predicted POI:  tk tunas bangsa\n",
      "\n",
      "\n",
      "Review: sla riyadi negara indonesia \n",
      "Original POI: negara indonesia \n",
      "Predicted POI:  wisma argo indonesia\n",
      "\n",
      "\n",
      "Review: bat internasional pt gajah mada no rw 1 \n",
      "Original POI: batara internasional pt \n",
      "Predicted POI:  pt delameta bilano\n",
      "\n",
      "\n",
      "Review: salon khu putri nogo depok \n",
      "Original POI: salon khusus putri \n",
      "Predicted POI:  salon salon\n",
      "\n",
      "\n",
      "Review: war ijo dr wahidin sudiroh \n",
      "Original POI: warung ijo \n",
      "Predicted POI:  warung mbah\n",
      "\n",
      "\n",
      "Review: amb war kopi \n",
      "Original POI: warung kopi \n",
      "Predicted POI:  warung kopi\n",
      "\n",
      "\n",
      "Review: gunung kembang c 20 sarolangun \n",
      "Original POI: syamsuddin c 20 \n",
      "Predicted POI:  pasar\n",
      "\n",
      "\n",
      "Review: gpi moria tugu pahlawan \n",
      "Original POI: gpi \n",
      "Predicted POI:  pertamina\n",
      "\n",
      "\n",
      "Review: restu ibu cv kabup gladak anyar \n",
      "Original POI: restu ibu cv \n",
      "Predicted POI:  sinar abadi\n",
      "\n",
      "\n",
      "Review: bidan am keb raya pelab ratu bantargadung \n",
      "Original POI: bidan am keb \n",
      "Predicted POI:  bidan keb\n",
      "\n",
      "\n",
      "Review: apartement green bay tower borneo lt 9 cc jl pluit karang ayu barat no 1b pluit \n",
      "Original POI: green bay \n",
      "Predicted POI:  green bay\n",
      "\n",
      "\n",
      "Review: soto ayam rawon lamo pan sari 259 marga sari balikpapan barat \n",
      "Original POI: soto ayam rawon lamongan \n",
      "Predicted POI:  bakso ayam\n",
      "\n",
      "\n",
      "Review: r a kart pt indonesia cilandak \n",
      "Original POI: pt indonesia \n",
      "Predicted POI:  mie indonesia pt\n",
      "\n",
      "\n",
      "Review: focus collec kebon jeruk 6 maphar taman sari \n",
      "Original POI: focus collection \n",
      "Predicted POI:  collection\n",
      "\n",
      "\n",
      "Review: peri kem no 8 warung makan pelangi rw 1 pudakpayung \n",
      "Original POI: warung makan pelangi \n",
      "Predicted POI:  warung makan\n",
      "\n",
      "\n",
      "Review: kamp dukuh raya no 2 toko putra 6 kebayoran lama utara \n",
      "Original POI: toko putra \n",
      "Predicted POI:  toko sinar\n",
      "\n",
      "\n",
      "Review: p no 3 ban nug bang kumis regol wetan \n",
      "Original POI: banana nugget bang kumis \n",
      "Predicted POI:  penjahit kopi\n",
      "\n",
      "\n",
      "Review: jalan aria putra cluster paradise resort blok a2 no 18 serua indah ciputat tangsel \n",
      "Original POI: paradise resort \n",
      "Predicted POI:  paradise resort\n",
      "\n",
      "\n",
      "Review: jun komu mbl griya raya 1 3 sukawarna \n",
      "Original POI: junita komunika mbl \n",
      "Predicted POI:  club parfum\n",
      "\n",
      "\n",
      "Review: elektrindo perkasa utama pang perjua tanjung pura \n",
      "Original POI: elektrindo perkasa utama \n",
      "Predicted POI:  pt sejahtera\n",
      "\n",
      "\n",
      "Review: jl mh tham no 1 kb kebon melati seibu grand indon ug floor tanah abang \n",
      "Original POI: grand indonesia ug floor \n",
      "Predicted POI:  komplek bank\n",
      "\n",
      "\n",
      "Review: knalpot putra mahakam maha 11 klojen \n",
      "Original POI: knalpot putra mahakam \n",
      "Predicted POI:  pkbm putra\n",
      "\n",
      "\n",
      "Review: perum brd residence jl sunan gresik blok j1 no 8 rt 05 rw 20 pringrejo \n",
      "Original POI: perum brd residence \n",
      "Predicted POI:  perum queen residence\n",
      "\n",
      "\n",
      "Review: surya toserba jl karanggetas no 23 \n",
      "Original POI: surya toserba \n",
      "Predicted POI:  bumi moro\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(\"Review:\",seq2text(x_val[i]))\n",
    "    print(\"Original POI:\",seq2summary(y1_val[i]))\n",
    "    print(\"Predicted POI:\", decode_sequence(x_val[i].reshape(1,max_len_text)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.762227,
     "end_time": "2021-03-14T11:44:26.571977",
     "exception": false,
     "start_time": "2021-03-14T11:44:25.809750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 295.878004,
   "end_time": "2021-03-14T11:44:30.107125",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-14T11:39:34.229121",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
