{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "labeled-piano",
   "metadata": {},
   "source": [
    "### 数据读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-aruba",
   "metadata": {},
   "source": [
    "数据格式\n",
    "\n",
    "`raw_addr_words_train` and `raw_addr_words_test`: [['s.', 'par', '53', 'sidanegara', '4', 'cilacap', 'tengah', 'BOS'], ['angg', 'per,', 'baloi', 'indah', 'kel.', 'lubuk', 'baja', 'BOS'], ['asma', 'laun,', 'mand', 'imog,', 'BOS']]\n",
    "\n",
    "`POIs_words` and `streets_words`: [['kakap', 'raya', 'EOS'], ['jend', 'ahmad', 'yani', 'EOS'], ['raya', 'cila', 'kko', 'EOS'], ['EOS']]\n",
    "\n",
    "`sents` concatenates `raw_addr_words_train`, `raw_addr_words_test`, `POIs_words` and `streets_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "opposite-housing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['jl', 'kapuk', 'timur', 'delta', 'sili', 'iii', 'lippo', 'cika', '11', 'a', 'cicau', 'cikarang', 'pusat', 'BOS'], ['aye', 'jati', 'sampurna', 'BOS'], ['setu', 'siung', '119', 'rt', '5', '1', '13880', 'cipayung', 'BOS'], ['toko', 'dita', 'kertosono', 'BOS'], ['jl.', 'orde', 'baru', 'BOS'], ['raya', 'samb', 'gede', '299', 'toko', 'bb', 'kids', 'BOS'], ['kem', 'mel', 'raya', 'no', '4', 'bojong', 'rawalumbu', 'rt', '1', '36', 'rawalumbu', 'BOS'], ['tela', 'keuramat', 'kuta', 'alam', 'BOS'], ['gg.', 'i', 'wates', 'magersari', 'BOS'], ['bunga', 'ncole', 'ix', '2', 'BOS']]\n",
      "[['EOS'], ['prib', '3', 'EOS'], ['EOS'], ['perum', 'tata', 'resid', 'nirwana', 'EOS'], ['kakap', 'raya', 'EOS'], ['jend', 'ahmad', 'yani', 'EOS'], ['raya', 'cila', 'kko', 'EOS'], ['EOS'], ['EOS'], ['EOS']]\n",
      "950000\n",
      "max_sentence_length:  33\n",
      "max_poi_length:  21\n",
      "max_street_length:  16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train.head()\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test.head()\n",
    "\n",
    "# 处理raw_addr_words_train和raw_addr_words_test：split和append('BOS')\n",
    "raw_addr_train = df_train['raw_address'].tolist()\n",
    "raw_addr_test = df_test['raw_address'].tolist()\n",
    "raw_addr_words_train = []\n",
    "raw_addr_words_test = []\n",
    "for sentence in raw_addr_train:\n",
    "    sentence = sentence.replace(\",\", \"\")\n",
    "    sentence = sentence.split()\n",
    "    sentence.append('BOS')\n",
    "    raw_addr_words_train.append(sentence)\n",
    "for sentence in raw_addr_test:\n",
    "    sentence = sentence.replace(\",\", \"\")\n",
    "    sentence = sentence.split()\n",
    "    sentence.append('BOS')\n",
    "    raw_addr_words_test.append(sentence)\n",
    "\n",
    "# 把raw_addr_words_train和raw_addr_words_test都添加到sents\n",
    "sents = raw_addr_words_train\n",
    "sents.extend(raw_addr_words_test)\n",
    "\n",
    "# 求出input中sentence最长的长度，作为LSTM的time_step的参考\n",
    "max_sentence_length = 0\n",
    "for sentence in sents:\n",
    "    if len(sentence) > max_sentence_length:\n",
    "        max_sentence_length = len(sentence)\n",
    "\n",
    "# 处理POIs_words和streets_words：split和append('EOS')\n",
    "POIs_words = []\n",
    "streets_words = []\n",
    "for gt in df_train['POI/street'].tolist():\n",
    "    POI = gt.split('/', 1)[0]\n",
    "    street = gt.split('/', 1)[1]\n",
    "    POI = POI.split()\n",
    "    POI.append('EOS')\n",
    "    street = street.split()\n",
    "    street.append('EOS')\n",
    "    POIs_words.append(POI)\n",
    "    streets_words.append(street)\n",
    "\n",
    "# 求出ground_truth中sentence最长的长度，作为LSTM的time_step的参考\n",
    "max_poi_length = 0\n",
    "for sentence in POIs_words:\n",
    "    if len(sentence) > max_poi_length:\n",
    "        max_poi_length = len(sentence)\n",
    "max_street_length = 0\n",
    "for sentence in streets_words:\n",
    "    if len(sentence) > max_street_length:\n",
    "        max_street_length = len(sentence)\n",
    "\n",
    "# 把POIs_words和streets_words都添加到sents\n",
    "sents.extend(POIs_words)\n",
    "sents.extend(streets_words)\n",
    "\n",
    "print(sents[:10])\n",
    "print(sents[-10:])\n",
    "print(len(sents))\n",
    "print(\"max_sentence_length: \", max_sentence_length)\n",
    "print(\"max_poi_length: \", max_poi_length)\n",
    "print(\"max_street_length: \", max_street_length)\n",
    "# 综合考虑，time_step为64就够用\n",
    "\n",
    "# df_words = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-mortgage",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aerial-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# 一般小语料库的vector维度用200-300\n",
    "# sentences (iterable of iterables, optional) – 供训练的句子，可以使用简单的列表，但是对于大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。\n",
    "# corpus_file (str, optional) – LineSentence格式的语料库文件路径。\n",
    "# size (int, optional) – word向量的维度。\n",
    "# window (int, optional) – 一个句子中当前单词和被预测单词的最大距离。\n",
    "# min_count (int, optional) – 忽略词频小于此值的单词。\n",
    "# workers (int, optional) – 训练模型时使用的线程数。\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=sents, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "# model.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "absolute-gateway",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.7510145e-03 -4.7370796e-03  2.9411297e-03  4.8859799e-03\n",
      "  2.8488715e-03 -2.6719654e-03  3.6631844e-03  2.3172596e-03\n",
      "  2.5307646e-03 -2.2685218e-03  1.7151979e-04  3.7271092e-03\n",
      " -3.4571113e-03  1.6590450e-03 -1.7351321e-03  3.1850750e-03\n",
      " -4.2197909e-03  3.4302576e-03 -1.9578892e-03  3.5440323e-03\n",
      "  3.2221025e-04  1.5887568e-03  3.7067465e-03  1.3883339e-03\n",
      " -2.5920195e-03 -9.9590898e-04 -3.0845886e-03  3.4212242e-03\n",
      " -3.2305140e-03  2.5452189e-03 -4.3210844e-04  4.2407182e-03\n",
      "  4.6338956e-03 -8.9179759e-04 -4.9878997e-03 -4.3554399e-03\n",
      " -3.0786749e-03  2.9000796e-03 -2.4078618e-05  7.2029402e-04\n",
      " -1.3743703e-03 -1.7121357e-03  3.6064379e-03  4.6777684e-04\n",
      "  3.5667121e-03 -2.5101295e-03 -3.6852087e-03  2.1823538e-03\n",
      " -3.2539712e-04 -4.3696836e-03 -1.6757927e-03 -1.2041670e-03\n",
      " -2.9920565e-03  1.9305609e-03 -1.3281346e-03  1.8970171e-03\n",
      " -1.9230577e-03  2.5611972e-03  4.3853882e-04  1.3362422e-03\n",
      " -3.8731010e-03  6.1253677e-06  1.9853523e-03  2.8401176e-03\n",
      "  9.6844560e-05 -3.5904753e-03 -4.5014014e-03 -3.3059060e-03\n",
      "  1.5304619e-03  2.0367226e-03 -3.7803354e-03 -1.8748158e-03\n",
      " -2.7079561e-03  7.6408027e-04  4.9496009e-03  3.3538127e-03\n",
      "  2.5702813e-03 -2.5026947e-03 -2.8639436e-03  4.2784857e-03\n",
      "  8.7097380e-04  9.1154140e-04 -3.3053458e-03  2.7994537e-03\n",
      " -2.9586896e-03 -7.3931238e-04  4.6216282e-03  4.9392512e-04\n",
      "  2.4397911e-03 -1.2096054e-03 -3.2304409e-03 -5.8342074e-04\n",
      " -1.0712136e-03 -3.5419292e-03  4.7687837e-03 -2.1839654e-03\n",
      "  2.8180857e-03 -2.6254398e-03 -4.9907607e-03 -3.8439367e-04]\n",
      "[ 0.7792587  -0.3186208   1.0165286  -1.603868    0.42817318 -0.90469694\n",
      " -2.0678914  -1.4848149   0.2286407  -2.52006     0.47171396  2.1091104\n",
      "  1.6222702  -0.08316635 -1.1231011   0.6846474  -0.2827008  -1.0943142\n",
      " -1.4110168   0.5549853  -0.966215   -0.6828998   0.46646816  1.9229639\n",
      " -2.1462157   0.4239478  -1.2549305   1.6505171   2.8565779  -2.0522437\n",
      "  0.18164109  1.6721239  -2.7316792   3.4570484   1.2108082  -1.4716952\n",
      "  1.6487671  -0.6563985  -1.4167942  -0.77016044 -0.21866955  2.0197825\n",
      " -1.0340664   0.15723717 -0.76958585 -2.888719    1.2837292   0.13297488\n",
      " -0.98335874  0.3367005   0.9902607  -0.10821269  1.8216987   0.91281575\n",
      " -1.8765095   0.45936587 -1.9085863   0.5008153   0.07053899 -0.18178298\n",
      "  0.8085123  -0.49125186  1.0233546  -0.05948103 -0.55223954  0.5628268\n",
      " -0.54080194 -0.36044362 -0.21351016  1.0512823  -1.726516   -0.6820916\n",
      " -1.2083191  -1.438667   -0.69042826  0.7273151  -0.8149152   1.6022135\n",
      "  2.1129084   1.6585091  -0.5798151   0.47931093  2.135749   -0.22074656\n",
      " -1.8221859   1.6849693   0.6074943   0.34337264  2.779183    0.27650484\n",
      "  2.1257515  -0.25827563  0.1124647   0.38945076 -2.2956471   1.4768062\n",
      "  0.2852042   0.4184854  -1.2105635   0.59952265]\n",
      "0.34203595\n",
      "0.8780363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['<BOS>'])\n",
    "print(model.wv['<EOS>'])\n",
    "print(model.wv['kapuk'])\n",
    "print(model.similarity('kapuk','timur'))\n",
    "print(model.similarity('yaya','yayasan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-production",
   "metadata": {},
   "source": [
    "### 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "boolean-antarctica",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2VecKeyedVectors' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-550ca1d437ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mem_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_addr_words_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m4.0000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m5.1000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6.3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;34m'Embeddings parameter is expected to be 2-dimensional'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# raw_addr_words_train\n",
    "# raw_addr_words_test\n",
    "# POIs_words\n",
    "# streets_words\n",
    "\n",
    "def word2idx(word):\n",
    "    return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index2word[idx]\n",
    "\n",
    "# em_weights = model.wv\n",
    "# embedding = nn.Embedding.from_pretrained(em_weights)\n",
    "\n",
    "print(POIs)\n",
    "\n",
    "# max_sentence_length = 33\n",
    "# max_poi_length = 21\n",
    "# max_street_length = 16\n",
    "train_input = np.zeros([len(raw_addr_words_train), 64], dtype=np.int32)\n",
    "test_input = np.zeros([len(raw_addr_words_test), 64], dtype=np.int32)\n",
    "for i, sentence in enumerate(raw_addr_words_train):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        train_input[i, t] = word2idx(word)\n",
    "for i, sentence in enumerate(raw_addr_words_test):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        test_input[i, t] = word2idx(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-holiday",
   "metadata": {},
   "source": [
    "### 模型部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-messaging",
   "metadata": {},
   "source": [
    "torch.nn.LSTM(*args, **kwargs)\n",
    "- input_size – The number of expected features in the input x\n",
    "- hidden_size – The number of features in the hidden state h\n",
    "- num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "- bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "- batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "- dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "- bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "- proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "\n",
    "关于batch_first\n",
    "RNN的输入是(seq_len, batch_size, input_size)，batch_size位于第二维度\n",
    "https://www.jianshu.com/p/41c15d301542\n",
    "https://www.cnblogs.com/picassooo/p/13637140.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 1               \n",
    "BATCH_SIZE = 32\n",
    "TIME_STEP = 66          # rnn time step / image height # 设置为max_sentence_length的两倍，应该够用\n",
    "INPUT_SIZE = 100         # rnn input size / image width # 和前面的vector的长度保持一致\n",
    "HIDDEN_SIZE = 100\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch\n",
    "from torch import nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         \n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,         # rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(100, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255.   # shape (2000, 28, 28) value in range(0,1) 注意训练数据会自动规范化，但测试数据不会，所以这里要手动除以255，否则会导致训练不收敛\n",
    "test_y = test_data.test_labels.numpy()[:2000]    # covert to numpy array\n",
    "\n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):        # gives batch data\n",
    "        b_x = b_x.view(-1, INPUT_SIZE)              # reshape x to (batch, time_step, input_size)\n",
    "\n",
    "        output = rnn(b_x)                               # rnn output\n",
    "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            test_output = rnn(test_x)                   # (samples, time_step, input_size)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "\n",
    "# print 10 predictions from test data\n",
    "test_output = rnn(test_x[:10].view(-1, 28, 28))\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10], 'real number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-sunset",
   "metadata": {},
   "source": [
    "### Encoder-Decoder translation模型\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-polish",
   "metadata": {},
   "source": [
    "#### 辅助代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-folder",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang_name):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    df_train = pd.read_csv(\"train.csv\")\n",
    "    raw_addr_train = df_train['raw_address'].tolist()\n",
    "    # df_test = pd.read_csv(\"test.csv\")\n",
    "    # raw_addr_test = df_test['raw_address'].tolist()\n",
    "    POIs = []\n",
    "    # streets = []\n",
    "    for gt in df_train['POI/street'].tolist():\n",
    "        POI = gt.split('/', 1)[0]\n",
    "        POIs.append(POI)\n",
    "        # street = gt.split('/', 1)[1]\n",
    "        # streets.append(street)\n",
    "\n",
    "    # Combine addresses and POIs into pairs\n",
    "    pairs = []\n",
    "    for i in range(raw_addr_train):\n",
    "        pairs.append([raw_addr_train[i].replace(\",\", \"\"), POIs[i]])\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    lang = Lang(lang_name)\n",
    "\n",
    "    return lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang_name):\n",
    "    lang, pairs = readLangs(lang_name)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        lang.addSentence(pair[0])\n",
    "        lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang, pairs\n",
    "\n",
    "lang, pairs = prepareData('All_words')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-clearing",
   "metadata": {},
   "source": [
    "#### 核心代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden) # 上一个unit的output和hidden到这一个unit来\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有用这个模型\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
